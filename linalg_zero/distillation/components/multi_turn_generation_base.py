import json
import uuid
from typing import TYPE_CHECKING, Any

from distilabel.errors import DistilabelUserError
from distilabel.mixins.runtime_parameters import (
    RuntimeParameter,
    RuntimeParametersMixin,
)
from distilabel.models.llms.base import LLM
from pydantic import Field, PositiveInt, ValidationError

from linalg_zero.distillation.data import FunctionInvocationInfo, ThoughtSchema
from linalg_zero.grpo.verifiers.xml_parser import (
    XMLParser,
)
from linalg_zero.grpo.verify import parse_string, verify_answers
from linalg_zero.shared.lib import get_lib
from linalg_zero.shared.system_prompts import (
    ANSWER_CLOSE,
    ANSWER_OPEN,
    THINK_CLOSE,
    THINK_OPEN,
    TOOL_CALL_CLOSE,
    TOOL_CALL_OPEN,
)
from linalg_zero.shared.utils import get_logger

if TYPE_CHECKING:
    from distilabel.typing import ChatType


class MultiTurnWithToolUseBase(RuntimeParametersMixin):
    llm: LLM
    _logger = get_logger(__name__)

    n_turns: PositiveInt = Field(
        description="The number of turns to generate for the conversation.",
    )
    include_system_prompt: RuntimeParameter[bool] = Field(
        default=True,
        description="Whether to include the system prompt used in the generated conversation.",
    )
    library: list[str] = Field(
        description="The list of function names available for tool calls.",
    )
    system_prompt: RuntimeParameter[str] = Field(
        default=None,
        description="The system prompt to use for the generation.",
    )

    structured_output: RuntimeParameter[bool] = Field(
        default=False,
        description="Whether to use structured output for the generation.",
    )
    enable_hint_injection: RuntimeParameter[bool] = Field(
        default=True,
        description="If true (and immediate recovery is disabled), inject a system hint about the previous issue to guide the next turn.",
    )
    strict_format: RuntimeParameter[bool] = Field(
        default=True,
        description=f"If true, enforce strict '{THINK_OPEN} then {TOOL_CALL_OPEN}|{ANSWER_OPEN}' structure gate in parsing.",
    )

    def _prepare_inputs_for_instruction_generation(self, inputs: list[dict[str, Any]]) -> list["ChatType"]:
        prepared_inputs = []
        for data in inputs:
            conversation = []
            if self.system_prompt:
                conversation.append({"role": "system", "content": self.system_prompt})
            conversation.append({"role": "user", "content": data["query"]})

            prepared_inputs.append(conversation)

        return prepared_inputs

    def create_assistant_message(self, message: ThoughtSchema) -> dict[str, Any]:
        if message.completed:
            if message.final_answer is None:
                raise DistilabelUserError("final_answer cannot be None when completed=True")
            result = {
                "role": "assistant",
                "content": f"{THINK_OPEN}{message.thought}{THINK_CLOSE}\n\n{ANSWER_OPEN}{message.final_answer}{ANSWER_CLOSE}",
            }
        elif message.tool_call is not None:
            result = {
                "role": "assistant",
                "content": THINK_OPEN + message.thought + THINK_CLOSE,
                "tool_calls": [
                    {
                        "id": str(uuid.uuid4()),
                        "type": "function",
                        "function": {
                            "name": message.tool_call.name,
                            "arguments": json.dumps(message.tool_call.arguments),
                        },
                    }
                ],  # type: ignore[dict-item]
            }
        else:
            raise DistilabelUserError("Invalid instruction. Must be completed or have a tool call.")
        return result

    def create_tool_message(self, conversation: list["ChatType"], message: dict[str, Any]) -> dict[str, Any]:
        # NOTE: Find the last assistant message with tool calls. This only works for single-turn tool calls,
        # if we transition to multiple calls per turn, must match by name or position.
        tool_call_id = None
        for msg in reversed(conversation):
            if msg.get("role") == "assistant" and msg.get("tool_calls"):
                tool_call_id = msg.get("tool_calls", [{}])[0].get("id")
                break

        if tool_call_id is None:
            raise DistilabelUserError("No assistant message with tool_calls found for tool response")

        return {
            "role": "tool",
            "tool_call_id": tool_call_id,
            "name": message["function_name"],
            "content": message["execution_result"],
        }

    def _append_messages_to_conversations(  # noqa: C901
        self, role: str, messages: list[ThoughtSchema | None] | list[dict[str, str]], conversations: list["ChatType"]
    ) -> list["ChatType"]:
        """Appends the outputs generated by the LLM with the specified role to the conversations."""

        for message, conversation in zip(messages, conversations, strict=True):
            # The message may be None because of incomplete generations.
            if message is None:
                continue

            if role == "assistant":
                if isinstance(message, ThoughtSchema):
                    try:
                        # Prevent consecutive assistant messages without progress (no tool, not completed)
                        if message.tool_call is None and not message.completed:
                            # Check last non-system role; skip append if it's assistant to preserve alternation
                            last_role = next(
                                (m.get("role") for m in reversed(conversation) if m.get("role") != "system"), None
                            )
                            if last_role == "assistant":
                                continue
                        formatted_message = self.create_assistant_message(message)
                        conversation.append(formatted_message)
                    except DistilabelUserError:
                        # Skip malformed assistant messages - treat as generation failure
                        continue
                else:
                    continue
            elif role == "tool":
                if isinstance(message, dict):
                    try:
                        formatted_message = self.create_tool_message(conversation, message)
                        conversation.append(formatted_message)
                    except DistilabelUserError:
                        # Skip malformed tool messages - conversation state is corrupted
                        continue
                else:
                    continue
            else:
                raise DistilabelUserError(f"Invalid role: {role}. Must be 'assistant' or 'tool'.")
        return conversations

    def _prepare_conversation_outputs(
        self, conversations: list["ChatType"], final_answers: list[str], success_indices: list[bool]
    ) -> list[dict[str, Any]]:
        """Prepare the output conversation removing the system prompt if necessary.
        It will return a dictionary with a "messages" key."""
        outputs: list[dict[str, Any]] = []
        for conversation, final_answer, is_correct in zip(conversations, final_answers, success_indices, strict=True):
            if conversation is None:
                raise DistilabelUserError("Conversation is None. This should not happen.")

            if len(conversation) == 0:
                # Something went wrong with the `LLM` and it didn't generate any message
                outputs.append({"messages": []})
                continue

            conv_out = list(conversation)

            # Remove any mid-conversation system messages (keep only the very first if present)
            if len(conv_out) > 1:
                conv_out = [msg for i, msg in enumerate(conv_out) if not (msg.get("role") == "system" and i != 0)]

            # Drop initial system if requested
            if not self.include_system_prompt and conv_out[0]["role"] == "system":
                conv_out = conv_out[1:]

            output = {"messages": conv_out, "final_answer": final_answer, "is_correct": is_correct}
            outputs.append(output)
        return outputs

    def _generate_conversation_turn(
        self, conversations: list["ChatType"], active_indices: list[int]
    ) -> tuple[list["ChatType"], list[int], tuple[Any, ...], list[ThoughtSchema | None], dict[int, tuple[str, str]]]:
        # Generate an output for the conversations that are still active
        inputs = []
        for idx in active_indices:
            conversation = conversations[idx]
            inputs.append(conversation)

        outputs = self.llm.generate(
            inputs=inputs,
            num_generations=1,
            **self.llm.get_generation_kwargs(),
        )
        # Extract the single message from the conversation and the statistics in separate lists
        messages, statistics = zip(
            *[(output["generations"][0], output["statistics"]) for output in outputs],
            strict=True,
        )

        # Log potential truncation
        self._check_generation_truncation(statistics, active_indices)

        parser = XMLParser()
        seeded_messages: list[str | None] = []
        for msg in messages:
            if msg is None:
                seeded_messages.append(None)
            else:
                seeded_messages.append(parser.ensure_think_prefix(msg))

        # The parsed messages may contain `None`s if the LLM didn't generate a message.
        contexts = [conversations[idx] for idx in active_indices]
        parsed_active_msgs = self.extract_output(list(seeded_messages), contexts=contexts)

        # Inject hint for failed messages to guide next turn (no immediate extra LLM call)
        diagnostics: dict[int, tuple[str, str]] = {}
        if self.enable_hint_injection:
            diagnostics = self._inject_hints(conversations, active_indices, parsed_active_msgs, list(messages))

        active_conversations = [conversations[idx] for idx in active_indices]
        updated_conversations = self._append_messages_to_conversations(
            role="assistant",
            messages=parsed_active_msgs,
            conversations=active_conversations,
        )

        for idx, conv in zip(active_indices, updated_conversations, strict=True):
            conversations[idx] = conv

        # Keep samples active even if message is None (retry next turn)
        new_active_indices = [
            idx
            for idx, message in zip(active_indices, parsed_active_msgs, strict=True)
            if (message is None or not message.completed)
        ]

        # Create full-sized array aligned with conversations list
        full_parsed_messages: list[ThoughtSchema | None] = [None] * len(conversations)
        for active_idx, parsed_msg in zip(active_indices, parsed_active_msgs, strict=True):
            full_parsed_messages[active_idx] = parsed_msg

        return conversations, new_active_indices, statistics, full_parsed_messages, diagnostics

    def _check_generation_truncation(self, statistics: tuple[Any, ...], active_indices: list[int]) -> None:
        """Check and log potential generation truncation based on token statistics."""
        generation_kwargs = self.llm.get_generation_kwargs()
        max_new_tokens = generation_kwargs["max_new_tokens"]
        if max_new_tokens is not None:
            for i, stats in enumerate(statistics):
                output_tokens = stats["output_tokens"]
                if output_tokens == max_new_tokens:
                    self._logger.warning(
                        f"Generation may have been truncated at max_new_tokens={max_new_tokens} "
                        f"for conversation {active_indices[i]} (output_tokens={output_tokens})"
                    )

    def _inject_hints(
        self,
        conversations: list["ChatType"],
        active_indices: list[int],
        parsed_active_msgs: list[ThoughtSchema | None],
        raw_messages: list[str | None],
    ) -> dict[int, tuple[str, str]]:
        """Append a brief system hint to conversations with malformed assistant outputs to guide the next turn.
        Returns a mapping from global sample index to (diagnostic reason, raw message).
        """
        reasons: dict[int, tuple[str, str]] = {}
        for local_idx, parsed in enumerate(parsed_active_msgs):
            if parsed is None or (parsed.tool_call is None and not parsed.completed):
                global_idx = active_indices[local_idx]
                conv = conversations[global_idx]
                raw = raw_messages[local_idx] or ""
                reason = self._diagnose_generation_issue(raw, context=conv)
                reasons[global_idx] = (reason, raw)
                hint = (
                    "Previous response issue: "
                    + reason
                    + f". Be explicit: write 2-4 steps in a single {THINK_OPEN} block. If not finished, emit exactly one {TOOL_CALL_OPEN} with valid JSON; otherwise emit exactly one {ANSWER_OPEN} containing only the result."
                )
                conv.append({"role": "system", "content": hint})
                self._logger.info(f"Injected recovery hint for turn {global_idx}: {reason}")

        return reasons

    def _execute_tool_calls(
        self, conversations: list["ChatType"], active_indices: list[int], parsed_messages: list[ThoughtSchema | None]
    ) -> tuple[list["ChatType"], list[int], dict[int, str], dict[int, int]]:
        # Filter only messages that have a valid tool call to execute
        exec_indices: list[int] = []
        exec_messages: list[ThoughtSchema] = []
        exec_conversations: list[ChatType] = []
        for idx in active_indices:
            message = parsed_messages[idx]
            if message is None or message.tool_call is None:
                # Skip malformed or tool-less messages; conversation remains active for next turn
                continue
            exec_indices.append(idx)
            exec_messages.append(message)
            exec_conversations.append(conversations[idx])

        if not exec_indices:
            return conversations, active_indices, {}, {}

        active_results, tool_statistics = self._execute(inputs=exec_messages, active_indices=exec_indices)

        updated_conversations = self._append_messages_to_conversations(
            role="tool",
            messages=active_results,
            conversations=exec_conversations,
        )

        for idx, conv in zip(exec_indices, updated_conversations, strict=True):
            conversations[idx] = conv

        # Determine tool errors by inspecting execution_result content
        error_counts: dict[int, int] = {}
        for local_pos, res in enumerate(active_results):
            if str(res.get("execution_result", "")).startswith("ERROR:"):
                global_idx = exec_indices[local_pos]
                error_counts[global_idx] = error_counts.get(global_idx, 0) + 1

        return conversations, active_indices, tool_statistics, error_counts

    def _execute(
        self, inputs: list[ThoughtSchema], active_indices: list[int]
    ) -> tuple[list[dict[str, str]], dict[int, str]]:
        results: list[dict[str, str]] = []
        statistics: dict[int, str] = {}

        for data, idx in zip(inputs, active_indices, strict=True):
            if data.tool_call is None:
                # Should not happen due to upstream filtering; skip safely
                continue

            name = data.tool_call.name
            arguments = data.tool_call.arguments

            # Track tool call frequency for this specific input
            statistics[idx] = name

            try:
                lib_functions = get_lib()
                if name not in lib_functions:
                    results.append({
                        "function_name": name,
                        "execution_result": f"ERROR: Function '{name}' not found in library",
                    })
                    continue

                result = lib_functions[name](**arguments)
                results.append({"function_name": name, "execution_result": str(result)})
            except Exception as exc:
                # Avoid stopping the pipeline on tool errors; propagate error text
                results.append({
                    "function_name": name,
                    "execution_result": f"ERROR: {type(exc).__name__}: {exc}",
                })

        return results, statistics

    def extract_output(self, messages: list[str | None], contexts: list[list[dict]]) -> list[ThoughtSchema | None]:
        """Extract the structured output from the messages."""
        result: list[ThoughtSchema | None] = []
        for ctx, message in zip(contexts, messages, strict=True):
            # Default placeholder to avoid None entries
            placeholder = ThoughtSchema(thought="", tool_call=None, final_answer=None, completed=False)
            if message is None:
                result.append(placeholder)
                continue
            try:
                parsed: ThoughtSchema | None = None
                if self.structured_output:
                    parsed = self.extract_structured_output(message, context=ctx)
                else:
                    parsed = self.extract_non_structured_output(message, context=ctx)
                if parsed is None:
                    parsed = placeholder
                result.append(parsed)
            except ValidationError:
                # Malformed JSON for structured path; keep placeholder to continue the flow
                result.append(placeholder)
        return result

    def extract_structured_output(self, message: str, context: list[dict]) -> ThoughtSchema | None:
        """Extract output from messages that enforce structured output."""
        parser = XMLParser()
        try:
            result = ThoughtSchema.model_validate_json(message)
        except ValidationError:
            return None

        # Enforce is_valid_think_then_tool_or_answer
        if result.tool_call is None and result.final_answer is None:
            return None

        # Enforce answer_policy_valid
        if result.final_answer:
            prev_is_tool_response = parser.is_last_msg_tool_call(context)

            if not prev_is_tool_response:
                return None

        tool_call = result.tool_call
        answer = result.final_answer

        # If both a tool and an answer appear, treat this as a tool-call step
        # and ignore the answer for this turn. In the unstructured setting,
        # is_valid_think_then_tool_or_answer enforces that tool_call and answer
        # can never be both present at the same time.
        if tool_call is not None and answer is not None:
            result.completed = False
            result.final_answer = None

        return result

    def extract_non_structured_output(self, message: str, context: list[dict]) -> ThoughtSchema | None:
        """Extract output from messages that do not enforce structured output."""
        parser = XMLParser()
        analysis = parser.analyze_message_in_context(context, message=message, tool_names=set(self.library))

        if self.strict_format and not bool(analysis["is_valid_think_then_tool_or_answer"]):
            return None

        if analysis["has_answer"] and not bool(analysis["answer_policy_valid"]):
            return None

        thought = analysis["thought"] or ""

        # Enforce a single tool call per turn: take only the last tool block
        tool_call: FunctionInvocationInfo | None = None
        tool_info = analysis["tool"]
        if tool_info and tool_info.get("json_valid"):
            tool_call = FunctionInvocationInfo(
                name=str(tool_info["name"]),
                arguments=dict(tool_info["arguments"]),
            )

        # Mark completion based on presence of answer
        answer = analysis["answer"]
        return ThoughtSchema(
            thought=thought,
            tool_call=tool_call,
            final_answer=answer,
            completed=answer is not None,
        )

    def _diagnose_generation_issue(self, msg: str | None, context: list[dict]) -> str:  # noqa: C901
        """Heuristic diagnosis of why a generation is unusable to guide recovery."""
        if msg is None or not str(msg).strip():
            return "empty generation"
        parser = XMLParser()
        msg = parser.ensure_think_prefix(msg) or ""
        analysis = parser.analyze_message_in_context(context, message=msg, tool_names=set(self.library))

        # Syntax checks
        if analysis["unclosed"]["think"]:
            return f"opened {THINK_OPEN} without matching {THINK_CLOSE}"

        if analysis["unclosed"]["answer"]:
            return f"opened {ANSWER_OPEN} without matching {ANSWER_CLOSE}"

        if analysis["unclosed"]["tool_call"]:
            return f"opened {TOOL_CALL_OPEN} without matching {TOOL_CALL_CLOSE}"

        if analysis["unopened"]["think"]:
            return f"closed {THINK_CLOSE} without matching {THINK_OPEN}"

        if analysis["unopened"]["answer"]:
            return f"closed {ANSWER_CLOSE} without matching {ANSWER_OPEN}"

        if analysis["unopened"]["tool_call"]:
            return f"closed {TOOL_CALL_CLOSE} without matching {TOOL_CALL_OPEN}"

        # Policy-level checks
        if not analysis["has_tool_call"] and not analysis["has_answer"]:
            return "no tool call and no final answer"

        if (
            analysis["has_think"]
            and (analysis["has_tool_call"] or analysis["has_answer"])
            and not analysis["is_valid_think_then_tool_or_answer"]
        ):
            think_n = analysis.get("think_count", 0)
            tool_n = analysis.get("tool_call_count", 0)
            answer_n = analysis.get("answer_count", 0)
            if think_n != 1:
                return f"violates '{THINK_OPEN} then {TOOL_CALL_OPEN}|{ANSWER_OPEN}' structure: expected exactly one {THINK_OPEN} block, found {think_n}"
            if (tool_n + answer_n) != 1:
                return f"violates '{THINK_OPEN} then {TOOL_CALL_OPEN}|{ANSWER_OPEN}' structure: expected exactly one of {TOOL_CALL_OPEN} or {ANSWER_OPEN}, found {tool_n} {TOOL_CALL_OPEN} and {answer_n} {ANSWER_OPEN}"
            return f"violates '{THINK_OPEN} then {TOOL_CALL_OPEN}|{ANSWER_OPEN}' structure: each tag must be unique"

        if analysis["has_answer"] and not analysis["answer_policy_valid"]:
            return "final answer emitted without adjacent tool response"

        # Check the think/tool/answer blocks
        if not analysis["has_think"]:
            return f"missing {THINK_OPEN} block"

        # Check the tool block
        if analysis["code_fences_in_last_tool"]:
            return f"remove code fences from {TOOL_CALL_OPEN} content"

        if analysis["has_tool_call"]:
            tool = analysis["tool"]
            if not tool["json_valid"]:
                return "invalid tool JSON"

            if not isinstance(tool["arguments"], dict):
                return "invalid tool arguments"

            if tool["name"] not in self.library:
                return "unknown tool name"

        if not analysis["has_answer"]:
            return f"no {ANSWER_OPEN} block"

        # Other checks
        if analysis["stray_content"]:
            return "content outside allowed tags"

        return "unspecified formatting issue"

    def _generate_multi_turn_conversation(  # noqa: C901
        self, inputs: list[dict[str, Any]]
    ) -> tuple[list[dict[str, Any]], list[dict[str, Any]], list[dict[str, int]]]:
        conversations = self._prepare_inputs_for_instruction_generation(inputs)
        # Keep track of the active conversations, as it could happen that for some conversation
        # we can't generate the next turn because the `LLM` returned `None`.
        active_indices = list(range(len(conversations)))
        stats_gen: list[dict[int, Any]] = []
        stats_tools: list[dict[int, str]] = []
        final_answers: list[str | None] = [None] * len(conversations)
        malformed_counts: list[int] = [0] * len(conversations)
        tool_errors_total: list[int] = [0] * len(conversations)
        diagnostics_reasons: list[list[str]] = [[] for _ in range(len(conversations))]
        diagnostics_messages: list[list[str]] = [[] for _ in range(len(conversations))]
        for i in range(self.n_turns):
            if not active_indices:
                break

            # Store current active indices before they get updated
            current_active_indices = active_indices.copy()

            # Generate assistant-tool interaction
            (
                conversations,
                active_indices,
                statistics_generation,
                parsed_messages,
                diagnostics_map,
            ) = self._generate_conversation_turn(
                conversations=conversations,
                active_indices=active_indices,
            )

            # Use the original active indices to access the full parsed messages array
            for idx in current_active_indices:
                message = parsed_messages[idx]
                if message and message.completed:
                    final_answers[idx] = message.final_answer
                if message is None:
                    malformed_counts[idx] += 1

            # Record diagnostics for this turn
            for idx, (reason, raw) in diagnostics_map.items():
                diagnostics_reasons[idx].append(f"turn {i}: {reason}")
                diagnostics_messages[idx].append(raw)

            if i == (self.n_turns - 1):
                # Map tuple of stats back to global indices
                per_turn = dict(zip(current_active_indices, statistics_generation, strict=True))
                stats_gen.append(per_turn)
                break

            if not active_indices:
                break

            # Generate assistant message; execute only valid tool calls
            conversations, active_indices, statistics_tools, error_counts = self._execute_tool_calls(
                conversations=conversations, parsed_messages=parsed_messages, active_indices=active_indices
            )
            stats_tools.append(statistics_tools)
            per_turn = dict(zip(current_active_indices, statistics_generation, strict=True))
            stats_gen.append(per_turn)
            # Log tool errors if any
            if error_counts:
                self._logger.warning(f"Tool errors in turn {i}: {error_counts}")
            for idx, cnt in error_counts.items():
                tool_errors_total[idx] += cnt

        # Convert None values to empty strings instead of None
        final_answers_clean = [answer if answer is not None else "" for answer in final_answers]
        success_indices = self.check_final_answers(final_answers_clean, inputs)

        # Build per-sample generation stats list aligned to inputs (list of per-turn stats)
        gen_stats_list: list[list[Any]] = []
        for i in range(len(inputs)):
            per_sample_stats = [per_turn[i] for per_turn in stats_gen if i in per_turn]
            gen_stats_list.append(per_sample_stats)

        merged_stats_tools = self.merge_tool_stats(stats_tools, inputs=inputs)
        return (
            self._prepare_conversation_outputs(conversations, final_answers_clean, success_indices),
            [
                {
                    "gen_stats": gen_stats_list[i],
                    "malformed_turns": malformed_counts[i],
                    "tool_errors": tool_errors_total[i],
                    "diagnostics": diagnostics_reasons[i],
                    "diagnostic_messages": diagnostics_messages[i],
                }
                for i in range(len(inputs))
            ],
            merged_stats_tools,
        )

    def check_final_answers(self, final_answers: list[str], inputs: list[dict[str, Any]]) -> list[bool]:
        """Check if the final answers are correct."""
        is_correct = []
        for _input, lib_result in zip(inputs, final_answers, strict=True):
            ground_truth = json.loads(_input["ground_truth"])
            is_correct.append(verify_answers(ground_truth, parse_string(lib_result)))

        return is_correct

    def merge_tool_stats(
        self, batch_stats: list[dict[int, str]], inputs: list[dict[str, Any]]
    ) -> list[dict[str, int]]:
        """Merge the tool stats into a single dictionary."""
        merged_stats: list[dict[str, int]] = [{} for _ in range(len(inputs))]
        for turn_stats in batch_stats:
            for active_index, fn_name in turn_stats.items():
                merged_stats[active_index][fn_name] = merged_stats[active_index].get(fn_name, 0) + 1

        return merged_stats

    def _generate_with_pre_query_template(self, inputs: list[dict[str, Any]]) -> list[dict[str, Any]]:
        """Generate a list of instructions or conversations of the specified number of turns."""
        outputs, statistics_gens, statistics_tools = self._generate_multi_turn_conversation(inputs)
        generations = []
        for input_data, output, stats_gen, stats_tools in zip(
            inputs, outputs, statistics_gens, statistics_tools, strict=True
        ):
            generation = {
                **input_data,
                **output,
                "model_name": self.llm.model_name,
            }
            generation["distilabel_metadata"] = {
                f"statistics_gen_{self.name}": stats_gen.get("gen_stats", []),
                f"statistics_tools_{self.name}": stats_tools if isinstance(stats_tools, dict) else {},
                f"malformed_turns_{self.name}": int(stats_gen.get("malformed_turns", 0) or 0),
                f"tool_errors_{self.name}": int(stats_gen.get("tool_errors", 0) or 0),
                f"tool_calls_total_{self.name}": int(
                    sum(stats_tools.values()) if isinstance(stats_tools, dict) else 0
                ),
                f"diagnostics_{self.name}": list(stats_gen.get("diagnostics", [])),
                f"diagnostic_messages_{self.name}": list(stats_gen.get("diagnostic_messages", [])),
            }
            generations.append(generation)
        return generations
