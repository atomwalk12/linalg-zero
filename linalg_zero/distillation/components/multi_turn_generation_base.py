import json
import uuid
from collections.abc import Callable
from typing import TYPE_CHECKING, Any

from distilabel.errors import DistilabelUserError
from distilabel.mixins.runtime_parameters import (
    RuntimeParameter,
    RuntimeParametersMixin,
)
from distilabel.models.llms.base import LLM
from distilabel.utils.dicts import merge_dicts
from pydantic import Field, PositiveInt, ValidationError

from linalg_zero.distillation.data import ThoughtSchema
from linalg_zero.grpo.verify import parse_string, verify_answers

if TYPE_CHECKING:
    from distilabel.typing import ChatType


class MultiTurnWithToolUseBase(RuntimeParametersMixin):
    llm: LLM

    n_turns: PositiveInt = Field(
        description="The number of turns to generate for the conversation.",
    )
    include_system_prompt: RuntimeParameter[bool] = Field(
        default=True,
        description="Whether to include the system prompt used in the generated conversation.",
    )
    thought_schema: RuntimeParameter[type[ThoughtSchema]] = Field(
        default=None,
        description="The schema for the thought process.",
    )
    library: dict[str, Callable[..., Any]] = Field(
        description="The library of functions to use for the tool calls.",
    )
    system_prompt: RuntimeParameter[str] = Field(
        default=None,
        description="The system prompt to use for the generation.",
    )

    def _prepare_inputs_for_instruction_generation(self, inputs: list[dict[str, Any]]) -> list["ChatType"]:
        prepared_inputs = []
        for data in inputs:
            conversation = []
            if self.system_prompt:
                conversation.append({"role": "system", "content": self.system_prompt})
            conversation.append({"role": "user", "content": data["query"]})

            prepared_inputs.append(conversation)

        return prepared_inputs

    def create_assistant_message(self, message: ThoughtSchema) -> dict[str, Any]:
        if message.completed:
            if message.final_answer is None:
                raise DistilabelUserError("final_answer cannot be None when completed=True")
            result = {
                "role": "assistant",
                "content": f"<think>{message.thought}</think>\n\n<answer>{message.final_answer}</answer>",
            }
        elif message.tool_call is not None:
            result = {
                "role": "assistant",
                "content": "<think>" + message.thought + "</think>",
                "tool_calls": [
                    {
                        "id": str(uuid.uuid4()),
                        "type": "function",
                        "function": {
                            "name": message.tool_call.name,
                            "arguments": json.dumps(message.tool_call.arguments),
                        },
                    }
                ],  # type: ignore[dict-item]
            }
        else:
            raise DistilabelUserError("Invalid instruction. Must be completed or have a tool call.")
        return result

    def create_tool_message(self, conversation: list["ChatType"], message: dict[str, Any]) -> dict[str, Any]:
        # Find the last assistant message with tool calls
        tool_call_id = None
        for msg in reversed(conversation):
            if msg.get("role") == "assistant" and msg.get("tool_calls"):
                tool_call_id = msg.get("tool_calls", [{}])[0].get("id")
                break

        if tool_call_id is None:
            raise DistilabelUserError("No assistant message with tool_calls found for tool response")

        return {
            "role": "tool",
            "tool_call_id": tool_call_id,
            "name": message["function_name"],
            "content": message["execution_result"],
        }

    def _append_messages_to_conversations(
        self, role: str, messages: list[ThoughtSchema | None] | list[dict[str, str]], conversations: list["ChatType"]
    ) -> list["ChatType"]:
        """Appends the outputs generated by the LLM with the specified role to the conversations."""

        for message, conversation in zip(messages, conversations, strict=True):
            # The message may be None because of incomplete generations.
            if message is None:
                continue

            if role == "assistant":
                if isinstance(message, ThoughtSchema):
                    try:
                        formatted_message = self.create_assistant_message(message)
                        conversation.append(formatted_message)
                    except DistilabelUserError:
                        # Skip malformed assistant messages - treat as generation failure
                        continue
                else:
                    continue
            elif role == "tool":
                if isinstance(message, dict):
                    try:
                        formatted_message = self.create_tool_message(conversation, message)
                        conversation.append(formatted_message)
                    except DistilabelUserError:
                        # Skip malformed tool messages - conversation state is corrupted
                        continue
                else:
                    continue
            else:
                raise DistilabelUserError(f"Invalid role: {role}. Must be 'assistant' or 'tool'.")
        return conversations

    def _prepare_conversation_outputs(
        self, conversations: list["ChatType"], final_answers: list[str], success_indices: list[bool]
    ) -> list[dict[str, Any]]:
        """Prepare the output conversation removing the system prompt if necessary.
        It will return a dictionary with a "conversation" key."""
        outputs: list[dict[str, Any]] = []
        for conversation, final_answer, is_correct in zip(conversations, final_answers, success_indices, strict=True):
            if conversation is None:
                raise DistilabelUserError("Conversation is None. This should not happen.")

            if len(conversation) == 0:
                # Something went wrong with the `LLM` and it didn't generate any message
                outputs.append({"conversation": []})
                continue
            if not self.include_system_prompt and conversation[0]["role"] == "system":
                conversation.pop(0)

            output = {"conversation": conversation, "final_answer": final_answer, "is_correct": is_correct}
            outputs.append(output)
        return outputs

    def _generate_conversation_turn(
        self, conversations: list["ChatType"], active_indices: list[int]
    ) -> tuple[list["ChatType"], list[int], tuple[Any, ...], list[ThoughtSchema | None]]:
        # Generate an output for the conversations that are still active
        inputs = []
        for idx in active_indices:
            conversation = conversations[idx]
            inputs.append(conversation)

        outputs = self.llm.generate(
            inputs=inputs,
            num_generations=1,
            **self.llm.get_generation_kwargs(),
        )
        # Extract the single message from the conversation and the statistics in separate lists
        messages, statistics = zip(
            *[(output["generations"][0], output["statistics"]) for output in outputs],
            strict=True,  # debug(atom)
        )

        # The parsed messages may contain `None`s if the LLM didn't generate a message.
        parsed_active_msgs = self.extract_structured_output(list(messages), self.thought_schema)

        active_conversations = [conversations[idx] for idx in active_indices]
        updated_conversations = self._append_messages_to_conversations(
            role="assistant",
            messages=parsed_active_msgs,
            conversations=active_conversations,
        )

        for idx, conv in zip(active_indices, updated_conversations, strict=True):
            conversations[idx] = conv

        new_active_indices = [
            idx
            for idx, output, message in zip(active_indices, outputs, parsed_active_msgs, strict=True)
            if output is not None and message is not None and not message.completed
        ]

        # Create full-sized array aligned with conversations list
        full_parsed_messages: list[ThoughtSchema | None] = [None] * len(conversations)
        for active_idx, parsed_msg in zip(active_indices, parsed_active_msgs, strict=True):
            full_parsed_messages[active_idx] = parsed_msg

        return conversations, new_active_indices, statistics, full_parsed_messages

    def _execute_tool_calls(
        self, conversations: list["ChatType"], active_indices: list[int], parsed_messages: list[ThoughtSchema | None]
    ) -> tuple[list["ChatType"], list[int], dict[int, str]]:
        active_conversations = []
        active_parsed_messages = []
        for idx in active_indices:
            # Use structured output for tool calls
            active_conversations.append(conversations[idx])
            message = parsed_messages[idx]
            if message is None:
                raise DistilabelUserError(f"Message at index {idx} is None, but should never be None at this step")
            active_parsed_messages.append(message)

        active_results, tool_statistics = self._execute(inputs=active_parsed_messages, active_indices=active_indices)

        updated_conversations = self._append_messages_to_conversations(
            role="tool",
            messages=active_results,
            conversations=active_conversations,
        )

        for idx, conv in zip(active_indices, updated_conversations, strict=True):
            conversations[idx] = conv

        return conversations, active_indices, tool_statistics

    def _execute(
        self, inputs: list[ThoughtSchema], active_indices: list[int]
    ) -> tuple[list[dict[str, str]], dict[int, str]]:
        results: list[dict[str, str]] = []
        statistics: dict[int, str] = {}

        for data, idx in zip(inputs, active_indices, strict=True):
            if data.tool_call is None:
                raise DistilabelUserError("Tool call is None, but should never be None at this step")

            name = data.tool_call.name
            arguments = data.tool_call.arguments

            # Track tool call frequency for this specific input
            statistics[idx] = name

            result = self.library[name](**arguments)
            results.append({"function_name": name, "execution_result": str(result)})

        return results, statistics

    def extract_structured_output(
        self, messages: list[str | None], schema: type[ThoughtSchema]
    ) -> list[ThoughtSchema | None]:
        """Extract the structured output from the messages."""
        result: list[ThoughtSchema | None] = []
        for message in messages:
            if message is None:
                result.append(None)
            else:
                try:
                    result.append(schema.model_validate_json(message))
                except ValidationError:
                    # The message is malformed, so we skip it
                    result.append(None)
        return result

    def _generate_multi_turn_conversation(
        self, inputs: list[dict[str, Any]]
    ) -> tuple[list[dict[str, Any]], list[dict[str, Any]], list[dict[str, int]]]:
        conversations = self._prepare_inputs_for_instruction_generation(inputs)
        # Keep track of the active conversations, as it could happen that for some conversation
        # we can't generate the next turn because the `LLM` returned `None`.
        active_indices = list(range(len(conversations)))
        stats_gen = []
        stats_tools = []
        final_answers: list[str | None] = [None] * len(conversations)
        for i in range(self.n_turns):
            if not active_indices:
                break

            # Store current active indices before they get updated
            current_active_indices = active_indices.copy()

            # Generate assistant-tool interaction
            conversations, active_indices, statistics_generation, parsed_messages = self._generate_conversation_turn(
                conversations=conversations,
                active_indices=active_indices,
            )

            # Use the original active indices to access the full parsed messages array
            for idx in current_active_indices:
                message = parsed_messages[idx]
                if message and message.completed:
                    final_answers[idx] = message.final_answer

            if i == (self.n_turns - 1):
                statistics = merge_dicts(*[statistics_generation])
                stats_gen.append(statistics)
                break

            if not active_indices:
                break

            # Generate assistant message
            conversations, active_indices, statistics_tools = self._execute_tool_calls(
                conversations=conversations, parsed_messages=parsed_messages, active_indices=active_indices
            )
            stats_tools.append(statistics_tools)
            stats_gen.append(statistics_generation)

        # Convert None values to empty strings instead of None
        final_answers_clean = [answer if answer is not None else "" for answer in final_answers]
        success_indices = self.check_final_answers(final_answers_clean, inputs)

        # Merge the dicts again at the conversation level
        merged_stats_gen = merge_dicts(*stats_gen)
        merged_stats_tools = self.merge_tool_stats(stats_tools, inputs=inputs)
        return (
            self._prepare_conversation_outputs(conversations, final_answers_clean, success_indices),
            merged_stats_gen,
            merged_stats_tools,
        )

    def check_final_answers(self, final_answers: list[str], inputs: list[dict[str, Any]]) -> list[bool]:
        """Check if the final answers are correct."""
        is_correct = []
        for _input, lib_result in zip(inputs, final_answers, strict=True):
            # TODO[atom]: perhaps this could be changed to store a list, float or int directly
            is_correct.append(verify_answers(_input["ground_truth"], parse_string(lib_result)))

        return is_correct

    def merge_tool_stats(
        self, batch_stats: list[dict[int, str]], inputs: list[dict[str, Any]]
    ) -> list[dict[str, int]]:
        """Merge the tool stats into a single dictionary."""
        merged_stats: list[dict[str, int]] = [{} for _ in range(len(inputs))]
        for turn_stats in batch_stats:
            for active_index, fn_name in turn_stats.items():
                merged_stats[active_index][fn_name] = merged_stats[active_index].get(fn_name, 0) + 1

        return merged_stats

    def _generate_with_pre_query_template(self, inputs: list[dict[str, Any]]) -> list[dict[str, Any]]:
        """Generate a list of instructions or conversations of the specified number of turns."""
        outputs, statistics_gens, statistics_tools = self._generate_multi_turn_conversation(inputs)
        generations = []
        for input_data, output, stats_gen, stats_tools in zip(
            inputs, outputs, statistics_gens, statistics_tools, strict=True
        ):
            generation = {
                **input_data,
                **output,
                "model_name": self.llm.model_name,
            }
            generation["distilabel_metadata"] = {
                f"statistics_gen_{self.name}": stats_gen,
                f"statistics_tools_{self.name}": stats_tools,
            }
            generations.append(generation)
        return generations
