# Distillation Configuration for llama.cpp inference

# Dataset parameters
dataset_name: "atomwalk12/linalgzero"
# dataset_name: "atomwalk12/linalg-distilled-debug"
debug_mode: True
dataset_config: null
take_n: 20

# Prompt parameters
prompt_column: "prompt"
prompt_template: "{{ instruction }}"

# Generation parameters
model_type: "default"
deterministic: true
max_new_tokens: 8192 #  32768
num_generations: 1

# Processing parameters
input_batch_size: 3
use_cache: true
client_replicas: 1
timeout: 600
retries: 2

# Output parameters
hf_output_dataset: "atomwalk12/linalgzero-distilled-debug"
argilla_output_dataset: "atomwalk12/linalgzero-distilled-debug"
private: false

# Model
model: "Qwen3-30B-A3B-Instruct-2507-UD-Q5_K_XL.gguf"
hf_model_repo_id: "unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF"
hf_pretrained_model_name_or_path: "Qwen/Qwen3-32B"

# ----------------------------

n_gpu_layers: 45 # 45
host: "0.0.0.0"
port: 8000
n_ctx: 8192
split_mode: 2

# Distillation parameters
n_turns: 6
